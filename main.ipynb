import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
- Get the data from [California Housing Price](https://www.kaggle.com/datasets/camnugent/california-housing-prices)
data = pd.read_csv("housing.csv")
## Data Exploring
data.head()
data.info()
data["ocean_proximity"].head()
data.isna().sum()

data["total_bedrooms"].value_counts()
data.shape
Problem:
- ocean_proximity: can not take the word to fit  our model as category -> preprocess
- total_bedrooms: has some missing values
# drop all the row contains na value 
data.dropna(inplace=True)
data.info()
data.isna().sum()
from sklearn.model_selection import train_test_split

# define X and y 
X = data.drop("median_house_value", axis=1)
y = data["median_house_value"]
X.head()
y.head()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
when we feel confident(finish tuning hyperparameter, training model...), we can use the model in which we has the most confidence to evaluate the test data
train_data = X_train.join(y_train)
train_data
train_data.hist(figsize=(15, 8));
train_data.hist();
# use sns to view correlation matrix
train_data.corr()
# plt.figure(figsize=(15,8))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu");
plt.figure(figsize=(15,8))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu");
We can see that `median_income` and especially related to `median_house_value` 
## Data Preprocessing
- all the plots tend to be right skew -> not a nice gaussian belt curve

> why do we need to use np.log

We often use np.log() function in data analysis and machine learning to transform data that has a large range of values or contains outliers into a more normalized form. This is because the logarithmic transformation tends to compress larger values and expand smaller values, making the distribution of the data more symmetrical and the variance more uniform.
train_data["total_rooms"] = np.log(train_data["total_rooms"] + 1)
train_data["total_bedrooms"] = np.log(train_data["total_bedrooms"] + 1)
train_data["population"] = np.log(train_data["population"] + 1)
train_data["households"] = np.log(train_data["households"] + 1)
train_data.hist(figsize=(15, 8));
train_data.ocean_proximity.value_counts()
We will turn `<1H Ocean`, `INLAND`... to new feature which contains 1 or 0(stands for true/false)

`pd.get_dummies` is a pandas function that is used to convert categorical variables into a set of binary variables (0 and 1) for use in machine learning models. It creates new columns in the DataFrame for each unique value of the categorical variable, and then assigns a 1 or 0 to each of those columns depending on whether the original value had that category or not.

For example, consider a DataFrame with a categorical variable 'color' that has three unique values: 'red', 'green', and 'blue'. After applying pd.get_dummies to the 'color' column, the resulting DataFrame will have three new columns 'color_red', 'color_green', and 'color_blue'. If a row originally had a value of 'red' in the 'color' column, then the 'color_red' column will have a value of 1 and the other two columns will have a value of 0.

pd.get_dummies is commonly used in machine learning preprocessing to convert categorical variables into a form that can be used in models that require numerical input data.




pd.get_dummies(train_data.ocean_proximity)
train_data = train_data.join(pd.get_dummies(train_data.ocean_proximity)).drop("ocean_proximity", axis=1)
plt.figure(figsize=(15,8))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu");
We can see that at `INLAND` the value is negative meaning that the price of the inland house you paid less than the median house value
plt.figure(figsize=(15, 8))
sns.scatterplot(x="latitude", y="longitude", data=train_data, hue="median_house_value", palette="coolwarm");
you can see that at the bottom left corner is the ocean and the price of the house near the ocean is `red` which means it really `expensive` compare to the house near the top right corner which near the land
## Feature Engineering
block with more household than usual -> has more room. so the `total_bedroom` alone can not say anything about the price

train_data["bedroom_ratio"] = train_data["total_bedrooms"] / train_data["total_rooms"]
train_data["household_rooms"] = train_data["total_rooms"] / train_data["households"]
plt.figure(figsize=(15,8))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu");
Now you can see that the bed_room ratio is related to median house value than the total badrooms

## Train models
from sklearn.linear_model import LinearRegression

X_train, y_train = train_data.drop(["median_house_value"], axis=1), train_data["median_house_value"]

reg = LinearRegression()

reg.fit(X_train, y_train)
def preprocessing_data(data):

    data["total_bedrooms"] = np.log(data["total_bedrooms"] + 1)
    data["total_rooms"] = np.log(data["total_rooms"] + 1)
    data["population"] = np.log(data["population"] + 1)
    data["households"] = np.log(data["households"] + 1)

    data = data.join(pd.get_dummies(data.ocean_proximity)).drop("ocean_proximity", axis=1)

    data["bedroom_ratio"] = data["total_bedrooms"] / data["total_rooms"]
    data["household_rooms"] = data["total_rooms"] / data["households"]
    return data
data = pd.read_csv("housing.csv")
data.dropna(inplace=True)
data.head()
data.info()
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

data = preprocessing_data(data)

np.random.seed(42)

X = data.drop("median_house_value", axis=1)
y = data["median_house_value"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

data = data.dropna()
plt.figure(figsize=(15, 8))
sns.heatmap(data.corr(), annot=True, cmap="YlGnBu");
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
model.score(X_test, y_test)
from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor()
forest.fit(X_train, y_train)
forest.score(X_test, y_test)
from sklearn.model_selection import GridSearchCV

param_grid = {
    "n_estimators": [100, 200],
    "min_samples_split": [2, 4],
    "max_depth": [None, 4, 8]
}

grid_search = GridSearchCV(forest, param_grid, cv=5,  scoring="neg_mean_squared_error", return_train_score=True)

grid_search.fit(X_train, y_train)
best_forest = grid_search.best_estimator_
best_forest.score(X_test, y_test)
